{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "---\n",
    "This repositori contains the solution for the Navigation challenge of the Deep Reinforcement Learning Nano Degree on Udacity.\n",
    "\n",
    "It consists of the following files:\n",
    "`Navigation.ipynb`\n",
    "`model.py`\n",
    "`dqn_agent.py`\n",
    "`checkpoint.pth`\n",
    "`README.md`\n",
    "`Report.ipynb`\n",
    "#### 1. Navigation.ipynb\n",
    "Contains the main structure of the program for loading the environment and training the dqn algorithm.\n",
    "#### 2. model.py\n",
    "Contains the code for the neural network for the dqn algorithm.\n",
    "The neural network is made up of two fully connected layer activated with ReLU activation function. Each of the laywers has 64 neurons. The input layer size is 37.\n",
    "#### 3. dqn_agent.py\n",
    "Contains the code for training the dqn algorithm with experience replay.\n",
    "#### 4. checkpoint.pth\n",
    "Contains the saved weights of the trained neural network from the solution to the environment.\n",
    "#### 5. README.md\n",
    "Contains information about how to setup the project, including installing the Unity environment.\n",
    "#### 6. Report.ipynb\n",
    "Contains the overall report for the solution for the environment.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "We implement a [Deep Q-Network](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) with the following extensions:\n",
    "\n",
    "* [Double learning](https://arxiv.org/abs/1509.06461)\n",
    "* [Dueling networks](https://arxiv.org/abs/1511.06581)\n",
    "* [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)\n",
    "\n",
    "A summary of famous extensions to the DQN framework can be found on the [Rainbow DQN paper](https://arxiv.org/abs/1710.02298) -- though we only implement the extensions listed above.\n",
    "\n",
    "Note that this implementation is built as an extension of the provided solution for the [DQN example](https://github.com/udacity/deep-reinforcement-learning/tree/master/dqn/solution), rather than implemented from scratch, but adds non-trivial technical extensions to it.\n",
    "\n",
    "\n",
    "### Details\n",
    "\n",
    "The agent keeps track of two DQNs: a local version, and a target version, one updated less often than the other.\n",
    "\n",
    "#### Dueling DQNs\n",
    "\n",
    "Each DQN is an Actor (Policy) model, mapping states to a list of action values for each action:\n",
    "\n",
    "    s -> [Q(s, a) for a in Actions]\n",
    "\n",
    "The network structure of the dueling networks generate independent estimates for the value function and the advantage function:\n",
    "\n",
    "    s -> Linear(state_size, fc1_units) -> ReLU -> x\n",
    "    x -> Linear(fc1_units, fc2_units / 2) -> ReLU -> Linear(fc2_units / 2, 1) -> V(s)\n",
    "    x -> Linear(fc1_units, fc2_units / 2) -> ReLU -> Linear(fc2_units / 2, action_size) -> A(s, a)\n",
    "\n",
    "The estimates for value and advantage are then combined as follows:\n",
    "\n",
    "$$ Q(s, a) = V(s) + \\left(A(s, a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a_i \\in \\mathcal{A}} A(s, a_i) \\right)$$\n",
    "     \n",
    "By default, 64 units are used on the first layer, and 64 units on the second layer, split between the value function network and the advantage function network.\n",
    "\n",
    "These networks are defined in `model.py`.\n",
    "\n",
    "#### Double DQNs\n",
    "\n",
    "On the update operation, rather than just using the local network to both select the best action and compute its estimated\n",
    "value, we use the local network to pick an action, and then use the target network to compute its estimated value:\n",
    "\n",
    "$$ Q_{expected} = R + \\gamma Q_{network}(s', \\underset{a \\in \\mathcal{A}}{\\operatorname{argmax}} Q_{local}(s', a)) $$\n",
    "\n",
    "This operation is performed at the `learn` method on the agent, in `dqn_agent.py`.\n",
    "\n",
    "# Solution\n",
    "The uploaded solution solves the environment in 322 episodes with the following hyperparameters:\n",
    "n_episodes=1000, eps_start=1.0, eps_end=0.08, eps_decay=0.990, state_size=37, action_size=4, seed=0 ,BUFFER_SIZE = int(1e5) ,BATCH_SIZE = 64,GAMMA = 0.99,TAU = 1e-3,LR = 5e-4 and UPDATE_EVERY = 4        \n",
    "\n",
    "![Training progress](images/Score.png)\n",
    "\n",
    "# Further Work\n",
    "Implement a solution with a double DQN, a dueling DQN and prioritized experience replay.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
